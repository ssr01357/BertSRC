{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, Dropout, Linear, MSELoss, Tanh\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSrcClassifier(BertPreTrainedModel):\n",
    "    \"\"\"BertSRC Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, config, mask_token_id: int, num_token_layer: int = 2):\n",
    "        super().__init__(config)\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.n_output_layer = num_token_layer\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout\n",
    "            if config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dense = Linear(config.hidden_size * num_token_layer, config.hidden_size)\n",
    "        self.activation = Tanh()\n",
    "        self.dropout = Dropout(classifier_dropout)\n",
    "        self.classifier = Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor = None,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        token_type_ids: torch.Tensor = None,\n",
    "        position_ids: torch.Tensor = None,\n",
    "        head_mask: torch.Tensor = None,\n",
    "        inputs_embeds: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "        output_attentions: bool = None,\n",
    "        output_hidden_states: bool = None,\n",
    "        return_dict: bool = None,\n",
    "    ):\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        assert 1 <= self.n_output_layer <= 3\n",
    "        if self.n_output_layer == 1:\n",
    "            output = outputs[0][0]\n",
    "        else:\n",
    "            check = input_ids == self.mask_token_id\n",
    "            if self.n_output_layer == 3:\n",
    "                check[:, 0] = True\n",
    "            output = torch.reshape(\n",
    "                outputs[0][check], (-1, self.n_output_layer * self.config.hidden_size)\n",
    "            )\n",
    "\n",
    "        output = self.dense(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (\n",
    "                    labels.dtype == torch.long or labels.dtype == torch.int\n",
    "                ):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "class BertsrcDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(classification_report(labels, preds, digits=3, target_names=label_encoder.classes_))\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abstracts = pd.read_csv(\"ChemProt_Corpus/chemprot_training/chemprot_training_abstracts.tsv\", sep=\"\\t\", names=[\"a_id\", \"title\", \"abstract\"])\n",
    "train_entities = pd.read_csv(\"ChemProt_Corpus/chemprot_training/chemprot_training_entities.tsv\", sep=\"\\t\", names=[\"a_id\", \"e_id\", \"type\", \"start\", \"end\", \"text\"])\n",
    "train_gs = pd.read_csv(\"ChemProt_Corpus/chemprot_training/chemprot_training_gold_standard.tsv\", sep=\"\\t\", names=[\"a_id\", \"relation\", \"e1\", \"e2\"])\n",
    "\n",
    "dev_abstracts = pd.read_csv(\"ChemProt_Corpus/chemprot_development/chemprot_development_abstracts.tsv\", sep=\"\\t\", names=[\"a_id\", \"title\", \"abstract\"])\n",
    "dev_entities = pd.read_csv(\"ChemProt_Corpus/chemprot_development/chemprot_development_entities.tsv\", sep=\"\\t\", names=[\"a_id\", \"e_id\", \"type\", \"start\", \"end\", \"text\"])\n",
    "dev_gs = pd.read_csv(\"ChemProt_Corpus/chemprot_development/chemprot_development_gold_standard.tsv\", sep=\"\\t\", names=[\"a_id\", \"relation\", \"e1\", \"e2\"])\n",
    "\n",
    "test_abstracts = pd.read_csv(\"ChemProt_Corpus/chemprot_test_gs/chemprot_test_abstracts_gs.tsv\", sep=\"\\t\", names=[\"a_id\", \"title\", \"abstract\"])\n",
    "test_entities = pd.read_csv(\"ChemProt_Corpus/chemprot_test_gs/chemprot_test_entities_gs.tsv\", sep=\"\\t\", names=[\"a_id\", \"e_id\", \"type\", \"start\", \"end\", \"text\"])\n",
    "test_gs = pd.read_csv(\"ChemProt_Corpus/chemprot_test_gs/chemprot_test_gold_standard.tsv\", sep=\"\\t\", names=[\"a_id\", \"relation\", \"e1\", \"e2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "abs_dict = train_abstracts.set_index(\"a_id\").to_dict()\n",
    "ent_dict = train_entities.set_index([\"a_id\", \"e_id\"]).to_dict()\n",
    "\n",
    "for i in range(len(train_gs)):\n",
    "    a_id = train_gs.loc[i, \"a_id\"]\n",
    "    e1_id = train_gs.loc[i, \"e1\"][5:]\n",
    "    e2_id = train_gs.loc[i, \"e2\"][5:]\n",
    "\n",
    "    text = abs_dict[\"title\"][a_id] + \" \" + abs_dict[\"abstract\"][a_id]\n",
    "    e1_start = ent_dict[\"start\"][(a_id, e1_id)]\n",
    "    e1_end = ent_dict[\"end\"][(a_id, e1_id)]\n",
    "    e2_start = ent_dict[\"start\"][(a_id, e2_id)]\n",
    "    e2_end = ent_dict[\"end\"][(a_id, e2_id)]\n",
    "\n",
    "    two_masked_input = text[:e1_start] + \"[MASK]\" + text[e1_end:] + \" [SEP] \" + text[:e2_start] + \"[MASK]\" + text[e2_end:]\n",
    "    train_x.append(two_masked_input)\n",
    "    train_y.append(train_gs.loc[i, \"relation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_x = []\n",
    "dev_y = []\n",
    "\n",
    "abs_dict = dev_abstracts.set_index(\"a_id\").to_dict()\n",
    "ent_dict = dev_entities.set_index([\"a_id\", \"e_id\"]).to_dict()\n",
    "\n",
    "for i in range(len(dev_gs)):\n",
    "    a_id = dev_gs.loc[i, \"a_id\"]\n",
    "    e1_id = dev_gs.loc[i, \"e1\"][5:]\n",
    "    e2_id = dev_gs.loc[i, \"e2\"][5:]\n",
    "\n",
    "    text = abs_dict[\"title\"][a_id] + \" \" + abs_dict[\"abstract\"][a_id]\n",
    "    e1_start = ent_dict[\"start\"][(a_id, e1_id)]\n",
    "    e1_end = ent_dict[\"end\"][(a_id, e1_id)]\n",
    "    e2_start = ent_dict[\"start\"][(a_id, e2_id)]\n",
    "    e2_end = ent_dict[\"end\"][(a_id, e2_id)]\n",
    "\n",
    "    two_masked_input = text[:e1_start] + \"[MASK]\" + text[e1_end:] + \" [SEP] \" + text[:e2_start] + \"[MASK]\" + text[e2_end:]\n",
    "    dev_x.append(two_masked_input)\n",
    "    dev_y.append(dev_gs.loc[i, \"relation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "abs_dict = test_abstracts.set_index(\"a_id\").to_dict()\n",
    "ent_dict = test_entities.set_index([\"a_id\", \"e_id\"]).to_dict()\n",
    "\n",
    "for i in range(len(test_gs)):\n",
    "    a_id = test_gs.loc[i, \"a_id\"]\n",
    "    e1_id = test_gs.loc[i, \"e1\"][5:]\n",
    "    e2_id = test_gs.loc[i, \"e2\"][5:]\n",
    "\n",
    "    text = abs_dict[\"title\"][a_id] + \" \" + abs_dict[\"abstract\"][a_id]\n",
    "    e1_start = ent_dict[\"start\"][(a_id, e1_id)]\n",
    "    e1_end = ent_dict[\"end\"][(a_id, e1_id)]\n",
    "    e2_start = ent_dict[\"start\"][(a_id, e2_id)]\n",
    "    e2_end = ent_dict[\"end\"][(a_id, e2_id)]\n",
    "\n",
    "    two_masked_input = text[:e1_start] + \"[MASK]\" + text[e1_end:] + \" [SEP] \" + text[:e2_start] + \"[MASK]\" + text[e2_end:]\n",
    "    test_x.append(two_masked_input)\n",
    "    test_y.append(test_gs.loc[i, \"relation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_y)\n",
    "\n",
    "train_y = torch.tensor(label_encoder.transform(train_y), dtype=torch.int).to(\"cuda\")\n",
    "dev_y = torch.tensor(label_encoder.transform(dev_y), dtype=torch.int).to(\"cuda\")\n",
    "test_y = torch.tensor(label_encoder.transform(test_y), dtype=torch.int).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertSrcClassifier: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertSrcClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertSrcClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertSrcClassifier were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['dense.weight', 'classifier.bias', 'classifier.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\")\n",
    "model = BertSrcClassifier.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\",\n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    num_token_layer=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_train = tokenizer(train_x, truncation=True, max_length=512)\n",
    "encodings_dev = tokenizer(dev_x, truncation=True, max_length=512)\n",
    "encodings_test = tokenizer(test_x, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BertsrcDataset(encodings_train, train_y)\n",
    "dev_dataset = BertsrcDataset(encodings_dev, dev_y)\n",
    "test_dataset = BertsrcDataset(encodings_test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    adam_epsilon=1e-8,\n",
    "    max_grad_norm=1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2416\n",
      "  Batch size = 256\n",
      "/tmp/ipykernel_150194/1688398046.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "/home/aiuser/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       CPR:3      0.886     0.878     0.882       550\n",
      "       CPR:4      0.934     0.921     0.928      1094\n",
      "       CPR:5      0.850     0.931     0.889       116\n",
      "       CPR:6      0.926     0.950     0.938       199\n",
      "       CPR:9      0.954     0.963     0.959       457\n",
      "\n",
      "    accuracy                          0.922      2416\n",
      "   macro avg      0.910     0.929     0.919      2416\n",
      "weighted avg      0.922     0.922     0.922      2416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 3458\n",
      "  Batch size = 256\n",
      "/tmp/ipykernel_150194/1688398046.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "/home/aiuser/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       CPR:3      0.890     0.812     0.849       665\n",
      "       CPR:4      0.929     0.945     0.937      1661\n",
      "       CPR:5      0.848     0.918     0.882       195\n",
      "       CPR:6      0.896     0.915     0.905       293\n",
      "       CPR:9      0.948     0.958     0.953       644\n",
      "\n",
      "    accuracy                          0.918      3458\n",
      "   macro avg      0.902     0.910     0.905      3458\n",
      "weighted avg      0.918     0.918     0.917      3458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('json')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8ec9cca2c87557e156fd8ef255634a318c57c49f04b33f39094cc06db516435"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
